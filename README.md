# FastCrapper - 雀魂对局数据爬虫

一个高效的雀魂对局数据爬虫工具，用于收集和分析雀魂玩家的对局数据。

## 设计思路

### 1. 核心功能
- 支持多种雀魂段位的数据爬取（王座、玉、金等）
- 可配置爬取页数范围和日期
- 自动处理并保存对局数据到CSV文件
- 支持并发爬取，提高效率

### 2. 性能优化
- **并发处理**：
  - 使用 ThreadPoolExecutor 实现多线程爬取
  - 动态调整线程数（最大3线程，不超过总页数）
  - 避免过多线程造成的资源浪费

- **请求优化**：
  - 添加2秒超时机制，防止请求卡死
  - 异常处理和重试机制
  - 避免重复请求相同数据

- **数据缓存**：
  - 实现数据缓冲区，每10条数据批量写入
  - 减少频繁的文件I/O操作
  - 程序结束时自动刷新缓冲区

### 3. 去重机制
- **三层去重策略**：
  1. 页面级别去重（processed_pages）
  2. 玩家级别去重（seen_players）
  3. 数据条目去重（seen_items）

## 使用说明

### 1. 环境要求
```python
Python 3.x
requests
```

### 2. 快速开始
```python
# 示例用法
if __name__ == '__main__':
    mode = 12        # 设置模式（王座: 16, 玉: 12, 金: 9, 王朝: 15, 玉东: 11, 金东: 8）
    start_page = 1   # 设置起始页
    end_page = 4     # 设置结束页
    date_str = '2025-04-08'  # 设置日期
    scraper = MahjongScraper(mode, start_page, end_page, date_str)
    scraper.run()
```

### 3. 输出文件
- 文件命名格式：`data_起始页-结束页_模式名_日期.csv`
- 包含字段：玩家名称（带分数）、对局链接
- 自动去重，确保数据唯一性

## 技术细节

### 1. 请求处理
- 使用 requests 库进行HTTP请求
- 添加请求头模拟浏览器行为
- 处理超时和异常情况

### 2. 数据结构
- **seen_items**: 用于存储已处理的数据条目
- **seen_players**: 跟踪已处理的玩家记录
- **processed_pages**: 记录已处理的页面
- **items_buffer**: 数据缓冲区，用于批量写入

### 3. 并发控制
- 根据页数范围动态调整线程数
- 使用线程池管理并发请求
- 实现页面级别的原子操作

### 4. 错误处理
- 捕获并记录网络请求异常
- 超时自动跳过，继续处理其他数据
- 确保程序稳定运行

## 注意事项
1. 建议根据实际需求调整以下参数：
   - `buffer_size`: 缓冲区大小
   - `timeout`: 请求超时时间
   - `max_workers`: 最大线程数

2. 避免过于频繁的请求，建议：
   - 合理设置页数范围
   - 适当调整并发数
   - 考虑添加请求延迟

## 更新日志
- 2024-04-08: 初始版本
  - 实现基本爬虫功能
  - 添加并发支持
  - 实现数据缓冲
  - 添加去重机制 